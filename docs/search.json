[
  {
    "objectID": "text_mining.html",
    "href": "text_mining.html",
    "title": "Text Mining",
    "section": "",
    "text": "Paradigmatic (similar context): A & B have paradigmatic relation if they can be substituted for each other.\n\nE.g., cat / dog\n\nSyntagmatic (correlated occurences ): A & B have syntagmatic relation if they can be combined with each other.\n\nE.g., cat / sit, car / drive\n\n\n\n\n\nFirst, compute a bag of words for a document in Vector Space Model (VSM). (E.g., [“eats”, “is”, “has”] = [4, 10, 5])\nProbability to randomly pick \\(w_i\\) from each document is\n\\[\nx_i = c(w_i, d1) / |d1|\n\\]\n\\[\ny_i = c(w_i, d2) / |d2|\n\\]\nwhere \\(c\\) denotes the count (term frequency) and \\(|d1|\\) denotes total counts of words in d1.\nWe represent each document as \\(d1 = (x_1,…,x_N)\\) and \\(d2 = (y_1,…,y_N)\\).\nThen, similarity between d1 and d2 is defined as\n\\[\nSim(d1,d2)=d1 \\cdot d2 = \\sum_{i=1}^{N} x_i y_i\n\\]\nWe call this Expected Overlap of Words in Context (EOWC).\nMoreover, we should\n\npenalize too frequent terms: Sublinear transformation of term frequency\n\n\\(TF(w,d) = \\frac{(k+1)c(w,d)}{c(w,d)+k}\\)\n\nreward matching a rare word: IDF term weighting\n\n\\(IDF(W) = \\log [(\\text{Total number of docs}+1)/ \\text{Doc Frequency}]\\)\n\n\nFinally, the similarity is redefined as\n\\[\nx_i = TF(w_i, d1) / |d1| \\\\\ny_i = TF(w_i, d2) / |d2| \\\\\nSim(d1,d2) = \\sum IDF(w_i) x_i y_i\n\\]\nAlso, we can discover syntagmatic relations from the highly weighted terms of\n\\[\n\\text{IDF-weighted d1} = (x_1 \\cdot IDF(w_1),...,x_N \\cdot IDF(w_N)).\n\\]\n\n\n\nWe can also find sytagmatic relations by Entropy \\(H(X)\\).\n\\[\nH(X_w) = \\sum_{v \\in {0,1}} -p(X_w=v) \\log_2 p(X_w=v)\n\\]\nwhere \\(0 \\log_2 0 = 0\\).\n\\(H(X_w)\\) is maximized (=1) when \\(p(X_w=1) = p(x_w=0) = 0.5\\) and minimized(=0) when [\\(p(X_w=1) = 1\\) and \\(p(x_w=0) = 0\\)] or [\\(p(X_w=1) = 0\\) and \\(p(x_w=0) = 1\\)].\nIn other words, high entropy words are harder to predict.\n\n\n\nConditional Entropy is defined as\n\\[\nH(X_{w1} | X_{w2}) = \\sum_{u \\in {0,1}} [p(X_{w2}=u) H(X_{w1} | X_{w2} = u)]\n\\]\n\\[\nH(X) \\geq H(X|Y)\n\\]\nWe can find syntagmatic relations by the following procedures.\n\nCompute conditional entropy for every other word.\nSort all the candidate words in ascending order of conditional entropy.\nTake the top-ranked candidates (with small conditional entropy) that have potential syntagmatic relations.\n\nNote, \\(H(X_{w1}|X_{w2})\\) and \\(H(X_{w1}|X_{w3})\\) is comparable and has the same upper bound \\(H(X_{w1})\\), while \\(H(X_{w1}|X_{w2})\\) and \\(H(X_{w3}|X_{w2})\\) are not. We have to compare the same probability of \\(w_1\\).\n\n\n\nMutual information is defined as\n\\[\nI(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)\n\\]\nwith the following properties:\n\n\\(I(X;Y) \\geq 0\\)\n\\(I(X;Y) = I(Y;X)\\)\n\\(I(X;Y) = 0\\) iff X&Y are independent\n\nWe can rewrite mutual information using KL-divergence as follows.\n\\[\nI(X_{w1};X_{w2}) = \\sum_{u \\in {0,1}} \\sum_{v \\in {0,1}} p(X_{w1} =u, X_{w2} = v) \\log_2 \\frac{p(X_{w1} = u, X_{w2} = v)}{p(X_w1 = u) p(X_{w2} = v)}\n\\]\nWe should add pseudo data so that no event has zero counts (Smoothing)."
  },
  {
    "objectID": "text_mining.html#topic-mining",
    "href": "text_mining.html#topic-mining",
    "title": "Text Mining",
    "section": "Topic mining",
    "text": "Topic mining\n\nInput\n\nA collection of N text documents \\(C={d_1,…,d_N}\\)\nVocabulary set: \\(V=\\{w_1,…,w_M\\}\\)\nNumber of topics \\(k\\)\n\nOutput\n\nk topics: \\({\\theta_1,…,\\theta_k}\\)\n\n\\(\\sum_{w \\in V} p(w|\\theta_i) = 1\\)\n\nCoverage of topics in each $d_i$: \\(\\{\\pi_{i1},…,\\pi_{ik}\\}\\)\n\n\\(\\pi_{ij}\\) denotes the probability of \\(d_i\\) covering topic \\(\\theta_j\\)\n\\(\\sum_{j=1}^k \\pi_{ij} = 1\\)\n\n\n\n\nUnigram Language Model\n\\[\np(w_1 w_2 ... w_n) = p(w_1)p(w_2)...p(w_n)\n\\]\n\n\nMaximum Likelihood\n\\[\n\\hat{\\theta} = \\arg \\max_\\theta P(X|\\theta)\n\\]\n\n\nBayesian estimation\n\\[\n\\hat{\\theta} = \\arg \\max_\\theta P(\\theta|X) = \\arg \\max_\\theta P(X|\\theta) P(\\theta)\n\\]\n\n\nMining one topic\n\nData\ndocument \\(d=x_1 x_2...x\\_{\\|d\\|}\\), \\(x_i \\in V={w_1,…,w_M}\\)\n\n\nModel\nUnigram LM \\(\\theta_i = p(w_i|\\theta)\\) where \\(i=1,…,M\\) and \\(\\theta_1+…+\\theta_M = 1\\)\n\n\nLikelihood function\n\\[\np(d|\\theta) = p(x_1|\\theta) \\times ... \\times p(x_{|d|}|\\theta) \\\\\n= p(w_1|\\theta)^{c(w_1,d)} \\times ... \\times p(w_M|\\theta)^{c(w_M,d)} \\\\\n= \\Pi_{i=1}^{M} p(w_i|\\theta)^{c(w_i,d)} \\\\\n= \\Pi_{i=1}^{M} \\theta_i^{c(w_i,d)}\n\\]\n\n\nML estimate (Log-Likelihood)\n\\[\n\\arg \\max_{\\theta_1,...,\\theta_M} \\sum_{i=1}^{M} c(w_i,d) \\log \\theta_i\n\\]\nThen, we can derive the following solution, using Lagrange multiplier approach.\n\\[\n\\hat{\\theta_i} = \\frac{c(w_i,d)}{|d|}\n\\]\n\n\n\nMixture of Two Unigram Language Models\nThis realizes factoring out background words.\n\nparameters \\(\\Lambda\\)\n\\[\np(w|\\theta_d), p(w|\\theta_B), p(\\theta_d), p(\\theta_B)\n\\]\nwhere \\(p(\\theta_d) + p(\\theta_B) = 1\\)\n\n\nLikelihood function\n\\[\np(d|\\Lambda) = \\Pi_{i-1}^{M} [p(\\theta_d) p(w_i|\\theta_d) + p(\\theta_B) p(w_i|\\theta_B)]^{c(w,d)}\n\\]\n\n\nML Estimate\n\\[\n\\Lambda^* = \\arg \\max_\\Lambda p(d|\\Lambda)\n\\]\nwhere\n\\[\n\\sum p(w_i|\\theta_d) = \\sum p(w_i|\\theta_B)=1 \\\\\np(\\theta_d)+p(\\theta_B)=1\n\\]\n\n\nThe Expectation-Maximization (EM) Algorithm\n\nInitialization\n\nInitialize \\(p(w|\\theta_d)\\) with random values.\n\nE-step\n\n\\[\np^{(n)}(z=0|w) = \\frac{p(\\theta_d) p^{(n)}(w|\\theta_d)}{p(\\theta_d) p^{(n)}(w|\\theta_d) + p(\\theta_B)p(w|\\theta_B)}\n\\]\nwhere \\(z\\) is a hidden variable and \\(z=0\\) and \\(z=1\\) denote a word comes from \\(\\theta_d\\) and \\(\\theta_B\\) respectively.\n\nM-step\n\n\\[\np^{(n+1)}(w|\\theta_d) = \\frac{c(w,d) p^{(n)}(z=0|w)}{\\sum_{w' \\in V}c(w',d)p^{(n)}(z=0|w')}\n\\]\n\nIterate E-step and M-step until the likelihood converges to a local maximum.\n\n\n\n\nProbabilistic Latent Semantic Analysis (PLSA)\n\nLikelihood Functions\n\\[\np_d(w) = \\lambda_B \\cdot p(w|\\theta_B) + (1-\\lambda_B)\\sum_{j=1}^k \\pi_{d,j} p(w|\\theta_j)\n\\]\n\\[\n\\log p(d) = \\sum_{w \\in V} c(w,d) \\log[\\lambda_B \\cdot p(w|\\theta_B) + (1-\\lambda_B)\\sum_{j=1}^k \\pi_{d,j}p(w|\\theta_j)]\n\\]\n\\[\n\\log p(C|\\Lambda) = \\sum_{d \\in C} \\sum_{w \\in V} c(w,d) \\log[\\lambda_B \\cdot p(w|\\theta_B) + (1-\\lambda_B)\\sum_{j=1}^k \\pi_{d,j}p(w|\\theta_j)]\n\\]\n\n\nEM algorithm\nHidden variable (topic indicator)\n\\[\nz_{d,w} \\in {B,1,2,...,k}\n\\]\n\nInitialization\nInitialize all unknown parameters randomly.\n\n\nE-step\nProbability that w in doc d is generated from topic \\(\\theta_j\\):\n\\[\np(Z_{d,w} = j) = \\frac{\\pi^{(n)}_{d,j} p^{(n)}(w|\\theta_j)}{\\sum_{j'=1}^k \\pi^{(n)}_{d,j'} p^{(n)}(w|\\theta_{j'})}\n\\]\nProbability that w in doc d is generated from background \\(\\theta_B\\):\n\\[\np(Z_{d,w} = B) = \\frac{\\lambda_B p(w|\\theta_B)}{\\lambda_B \\cdot p(w|\\theta_B) + (1-\\lambda_B)\\sum_{j=1}^k \\pi^{(n)}_{d,j} p^{(n)}(w|\\theta_{j})}\n\\]\n\n\nM-step\nRe-estimated probability of doc d covering topic \\(\\theta_j\\):\n\\[\n\\pi^{(n+1)}_{d,j} = \\frac{\\sum_{w \\in V}c(w,d)(1-p(z_{d,w}=B))p(z_{d,w}=j)}{\\sum_{j'}\\sum_{w \\in V}c(w,d)(1-p(z_{d,w}=B))p(z_{d,w}=j')}\n\\]\nRe-estimated probability of word w for topic \\(\\theta_j\\):\n\\[\np^{(n+1)}(w|\\theta_j) = \\frac{\\sum_{d \\in C}c(w,d)(1-p(z_{d,w}=B))p(z_{d,w}=j)}{\\sum_{w' \\in V}\\sum_{d \\in C}c(w',d)(1-p(z_{d,w'}=B))p(z_{d,w'}=j)}\n\\]\n\n\n\n\nPLSA with prior knowledge (User-controlled PLSA)\nMaximum a Posteriori (MAP) Estimate:\nAfter setting \\(\\Lambda\\) to encode all kinds of preferences and constraints, compute the following maximum likelihood estimate, using an EM algorithm:\n\\[\n\\Lambda^* = \\arg \\max_\\Lambda p(\\Lambda) p(Data| \\Lambda)\n\\]\nE.g., given prior of \\(p(w|\\theta_{j'})\\), one equation in M-step changes to\n\\[\np^{(n+1)}(w|\\theta_j) = \\frac{\\sum_{d \\in C}c(w,d)(1-p(z_{d,w}=B))p(z_{d,w}=j) + \\mu p(w|\\theta_{j'})}{\\sum_{w' \\in V}\\sum_{d \\in C}c(w',d)(1-p(z_{d,w'}=B))p(z_{d,w'}=j) + \\mu}\n\\]\n\n\nPLSA as a generative model (Latent Dirichlet Allocation)\nPLSA has the following deficiencies:\n\nnot a generative model\nmany parameters\n\nTo address these issues, LDA imposes a prior on \\(\\pi_d = (\\pi_{d,1},...,\\pi_{d,k})\\) and \\(\\theta_i = (p(w_1|\\theta_i),...,p(w_M|\\theta_i))\\) as follows.\n\\[\np(\\pi_d) = \\text{Dirichlet}(\\alpha),\n\\]\nwhere \\(\\alpha = (\\alpha_1,…,\\alpha_k)\\).\n\\[\np(\\theta_i) = \\text{Dirichlet}(\\beta)\n\\]\nwhere \\(\\beta=(\\beta_1,…,\\beta_M)\\).\nThen, parameters can be estimated using ML estimator:\n\\[\n(\\alpha,\\beta) = \\arg \\max_{\\alpha,\\beta} \\log p(C| \\alpha, \\beta)\n\\]"
  },
  {
    "objectID": "text_mining.html#text-clustering",
    "href": "text_mining.html#text-clustering",
    "title": "Text Mining",
    "section": "Text clustering",
    "text": "Text clustering\n\nGenerative probabilistic models\n\nData\na collection of documents \\(C={d_1,…,d_N}\\)\n\n\nModel\nmixture of k unigram LMs: \\(\\Lambda = ({\\theta_i}; {p(\\theta_i)})\\)\n\n\nLikelihood\n\\[\np(d|\\Lambda) = \\sum_{i=1}^k [p(\\theta_i) \\Pi_{j=1}^{|d|} p(x_j|\\theta_i)] \\\\\n= \\sum_{i=1}^k [p(\\theta_i) \\Pi_{w \\in V} p(w|\\theta_i)^{c(w,d)}]\n\\]\n\\[\np(C|\\Lambda) = \\Pi_{j=1}^N p(d_j|\\Lambda)\n\\]\n\n\nMaximum Likelihood estimate\n\\[\n\\Lambda^* = \\arg \\max_\\Lambda p(d|\\Lambda)\n\\]\n\n\nCluster document d belong to: \\(c_d\\)\ntwo ways to compute:\n\nLikelihood only\n\n\\[\nc_d = \\arg \\max_i p(d|\\theta_i)\n\\]\n\nLikelihood + prior \\(p(\\theta_i)\\) (Bayesian)\n\n\\[\nc_d = \\arg \\max_i p(d|\\theta_i) p(\\theta_i)\n\\]\n\n\nEM algorithm\n\nInitialize \\(\\Lambda\\) randomly.\nE-step\n\n\\[\np^{(n)} = (Z_d=i|d) \\propto p^{(n)}(\\theta_i) \\Pi_{w \\in V} p^{(n)} (w|\\theta_i)^{c(w,d)}\n\\]\n\nM-step\n\n\\[\np^{(n+1)}(\\theta_i) \\propto \\sum_{j=1}^N p^{(n)}(z_{d_j}=i|d_j)\n\\]\n\\[\np^{(n+1)}(w|\\theta_i) \\propto \\sum_{j=1}^N c(w,d_j) p^{(n)}(Z_{d_j}=1|d_j)\n\\]\n\nIterate E and M-step until the estimate converges.\n\n\n\n\nSimilarity-based approaches\n\nHierarchical Agglomerative Clustering (HAC)\nThis groups similar objects together in a bottom-up fashion.\nThree ways to compute group similarity:\n\nSingle-link algorithm: similarity of the closest pair (loose clusters)\nComplete-link algorithm: similarity of the farthest pair (tight clusters)\nAverage-link algorithm: average of similarity of all pairs (insensitive to outliers)\n\n\n\nk-means\n\nSelect k randomly selected vectors as the centroids of k clusters\nAssign every vector to the closest cluster\nRe-compute the centroid\nRepeat until the similarity-based objective function converges."
  },
  {
    "objectID": "text_mining.html#text-categorization",
    "href": "text_mining.html#text-categorization",
    "title": "Text Mining",
    "section": "Text Categorization",
    "text": "Text Categorization\n\nGenerative probabilistic models\nThey learn what data looks like in each.\nAttempt to model \\(p(X,Y)=p(Y)p(X|Y)\\) and compute \\(p(Y|X)\\).\nOften utilize machine learning to create a classifier.\n\nE.g., Naive Bayes\nGiven categories\n\\[\nT_1 = {d_{11},...,d_{1N_1}} \\\\\n...\\\\\nT_k = {d_{k1},...,d_{kN_1}},\n\\]\n\\[\np(\\theta_i) = \\frac{N_i}{\\sum_{j=1}^k N_j} \\propto |T_i|\n\\]\n\\[\np(w|\\theta_i) = \\frac{\\sum_{j=1}^{N_i}c(w,d_{ij})}{\\sum_{w' \\in V} \\sum_{j=1}^{N_i} c(w',d_{ij})} \\propto c(w,T_i)\n\\]\nIf smoothing to address data sparseness,\n\\[\np(\\theta_i) = \\frac{N_i + \\delta}{\\sum_{j=1}^k N_j + k \\delta}\n\\]\n\\[\np(w|\\theta_i) = \\frac{\\sum_{j=1}^{N_i}c(w,d_{ij}) + \\mu p(w|\\theta_B)}{\\sum_{w' \\in V} \\sum_{j=1}^{N_i} c(w',d_{ij}) + \\mu}\n\\]\nThen, the comparison of categories for document \\(d\\), we can apply logistic regression as follows.\n\\[\nscore(d) = \\log \\frac{p(\\theta_1|d)}{p(\\theta_2|d)} \\\\\n= \\log \\frac{p(\\theta_1) \\Pi_{w \\in V} p(w|\\theta_1)^{c(w,d)}}{p(\\theta_2) \\Pi_{w \\in V} p(w|\\theta_2)^{c(w,d)}} \\\\\n= \\log \\frac{p(\\theta_1)}{p(\\theta_2)} + \\sum_{w \\in V} c(w,d) \\log \\frac{p(w|\\theta_1)}{p(w|\\theta_2)} \\\\\n= \\beta_0 + \\sum f_i \\beta_i\n\\]\nE.g.,\nGiven\nCategory 1 T1:{d1=(w1w1w1w1w3w3)}\nCategory 2 T2:{d2=(w1w1w2w2w3w4)}\nd3=(w3,w4),\nthen,\n\\[\np(\\theta_1) = 1/2\n\\]\n\\[\np(\\theta_2) = 1/2\n\\]\n\\[\np(d3|\\theta_1) = 2/6 * 0/6 =  0\n\\]\n\\[\np(d3|\\theta_2) = 1/6 * 1/6 = 1/36\n\\]\nIf using Laplace smoothing,\n\\[\n|V| = 4\n\\]\n\\[\np(w3|\\theta_1) = 3/10\n\\]\n\\[\np(w4|\\theta_1) = 1/10\n\\]\n\\[\np(w3|\\theta_2) = 2/10\n\\]\n\\[\np(w4|\\theta_2) = 2/10\n\\]\n\\[\np(d3|\\theta_1) = 3/10 * 1/10\n\\]\n\\[\np(d3|\\theta_2) = 2/10 * 2/10\n\\]\nThe Naive Bayes predict (the posterior probability) is\n\\[\np(\\theta_2|d3) = \\frac{p(d3|\\theta_2)p(\\theta_2)}{p(d3)} \\\\\n= \\frac{p(d3|\\theta_2)p(\\theta_2)}{p(d3|\\theta_1)p(\\theta_1) + p(d3|\\theta_2)p(\\theta_2)}\n\\]\n\n\n\nDiscriminative approaches\nThey learn what features separate categories.\nAttempt to model \\(p(Y|X)\\) directly.\nE.g., Logistic regression, SVM, kNN\n\n\nEvaluation\n\\[\n\\text{Precision} = \\frac{TP}{TP+FP}\n\\]\n\\[\n\\text{Recall} = \\frac{TP}{TP+FN}\n\\]\n\\[\nF_\\beta = \\frac{(\\beta^2 + 1)P*R}{\\beta^2P+R}\n\\]\n\\[\nF_1 = \\frac{2PR}{P+R}\n\\]\n\nMacro Average over all the categories\nMacro Average over all the documents\nMicro-Averaging: precision and recall over all decisions across documents/categories"
  },
  {
    "objectID": "text_mining.html#opinion-mining-and-sentiment-analysis",
    "href": "text_mining.html#opinion-mining-and-sentiment-analysis",
    "title": "Text Mining",
    "section": "Opinion mining and sentiment analysis",
    "text": "Opinion mining and sentiment analysis\nFeature design affects categorization accuracy significantly.\n\nOrdinal Logistic Regression\ninput: document d\noutput: discrete rating \\(r \\in {1,2,…,k}\\)\nmodel: use \\(k-1\\) classifiers\n\\[\np(r \\geq j| X) = \\frac{e^{\\alpha_j + \\sum x_i \\beta_{ji}}}{e^{\\alpha_j + \\sum x_i \\beta_{ji}} + 1}\n\\]\nThe problems is as many parameters as $(k-1)*(M+1)$, although the positive/negative features are similar. To address this issue, we remodel as follows with \\(M+k-1\\) parmeters.\n\\[\np(r \\geq j| X) = \\frac{e^{\\alpha_j + \\sum x_i \\beta_{i}}}{e^{\\alpha_j + \\sum x_i \\beta_{i}} + 1}\n\\]\n\n\nLatent Aspect Rating Analysis\nRefer to https://www.cs.virginia.edu/~hw5x/paper/rp166f-wang.pdf.\n\nData\n\na set of review documents with overall ratings: \\(C=\\{(d,r_d)\\}\\)\nd is pre-segmented into k review aspect segments (price, food, etc.)\n\\(c_i(w,d)\\) denotes the count of word w in aspect segment i\n\nModel\n\npredict each aspect rating\n\n\\(r_i(d) = \\sum_{w \\in V} c_i(w,d)\\beta_{i,w}\\)\n\nOverall rating can be calculated as\n\n\\(r_d \\sim N(\\sum \\alpha_i(d) * r_i(d), \\delta^2)\\)\nwhere \\(\\alpha(d) \\sim N(\\mu, \\Sigma)\\) (Multivariate Gaussian Prior)\n\nMaximum Likelihood Estimate\n\n\\(\\Lambda = (\\beta, \\mu, \\Sigma, \\delta)\\)\n\\(\\Lambda^* = \\arg \\max_\\Lambda \\Pi_{ d\\in C} p(r_d|d,\\Lambda)\\)\n\nAspect Weights\n\n\\(\\alpha(d)^* = \\arg \\max_{\\alpha(d)} p(\\alpha(d)| \\mu, \\Sigma) * p(r_d | d, \\beta, \\delta^2, \\alpha(d))\\)"
  },
  {
    "objectID": "text_mining.html#contextual-text-mining",
    "href": "text_mining.html#contextual-text-mining",
    "title": "Text Mining",
    "section": "Contextual text Mining",
    "text": "Contextual text Mining\n\nContextual Probabilistic Latent Semantic Analysis (CPLSA\n\nExplicitly add interesting context variables into a generative model\n\n\n\nNetwork Supervised Topic Modeling (NetPLSA)\nAdd network-induced regularizers to the likelihood objective function.\n\\[\n\\Lambda^* = \\arg \\max_\\Lambda f(p(\\text{TextData}|\\Lambda), r(\\Lambda,\\text{Network}))\n\\]\n\n\nMining Causal Topics with Time Series Supervision\n\ninput\n\nTime series\nText data\n\noutput\n\nTopics with strong correlations with the time series (causal topics)\n\n\nThe Granger Causality Test is a statistical hypothesis test for “precedence” (not causality technically)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Introduction to Text Retrieval and Mining",
    "section": "",
    "text": "This is a note for the lectures CS 410 Text Information Systems."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "text_retrieval.html#text-access",
    "href": "text_retrieval.html#text-access",
    "title": "Text Retrieval",
    "section": "Text Access",
    "text": "Text Access\n\nPull: search engines\n\nQuerying\nBrowsing\n\nPush: recommender systems"
  },
  {
    "objectID": "text_retrieval.html#text-retrieval-problem",
    "href": "text_retrieval.html#text-retrieval-problem",
    "title": "Text Retrieval",
    "section": "Text Retrieval Problem",
    "text": "Text Retrieval Problem\nSearch engine system returns relevant documents to users from collection of text documents.\nOften called “information retrieval (IR),” but IR is much broader than text retrieval.\nTR is an empirically defined problem and cannot mathematically prove one method to be the best.\nWe have two methods, but generally prefer ranking.\n\nDocument selection\n\nReturn the docs which system decides as relevant (absolute relevance)\nCons\n\nThe classifier can be inaccurate\n\n“Over-constrained” query gets no relevant docs.\n“Under-constrained” query gets too many docs.\n\nCannot see the degree of relevance or cannot prioritize docs\n\n\nDocument ranking\n\nReturn the docs with a relevance measure exceeding a cutoff (relative relevance)"
  },
  {
    "objectID": "text_retrieval.html#text-retrieval-methods",
    "href": "text_retrieval.html#text-retrieval-methods",
    "title": "Text Retrieval",
    "section": "Text Retrieval Methods",
    "text": "Text Retrieval Methods\nHere are four funcitons to calculate relevance \\(f(q,d)\\), all of which tend to result in similar ranking functions.\n\nSimilarity-based models: \\(f(q,d) = similarity(q,d)\\)\n\nVector space model\n\nProbabilistic model: \\(f(d,q) = p(R=1|d,q)\\)\nProbabilistic inference model: \\(f(q,d) = p(d \\rightarrow q)\\)\nAxiomatic model: f(q,d) must satisfy a set of constraints\n\nThree variables:\n\nTerm Frequency (TF): \\(c(term, d)\\)\nDocument Length: \\(|d|\\)\nDocument Frequency (df): \\(P(term | entire \\space collection)\\).\n\nThe Best models:\n\nPivoted length normalization\nBM25: most popular\nQuery likelihood\nPL2"
  },
  {
    "objectID": "text_retrieval.html#vector-space-model-vsm",
    "href": "text_retrieval.html#vector-space-model-vsm",
    "title": "Text Retrieval",
    "section": "Vector Space Model (VSM)",
    "text": "Vector Space Model (VSM)\nThis model adopts similarity-based model to calculate relevance.\n\nTerm\n\nEach term defines one dimension\nN terms define an N-dimensional space\n\nQuery: \\(q = (x_1, …, x_N)\\) is query term weight.\nDoc: \\(d=(y_1,…,y_N)\\) is doc term weight.\n\n\nSimplest VSM\n\nDimension\n\nBag of Words: \\(V=(w_1,…,w_n)\\)\nA text (such as a sentence or a document) is represented as the bag of its words.\nIt focuses on individual words.\nIt discards\n\nphrases by multiple words\nword ordering\n\n\nWeight\n\nBit Vector: \\((x_i, y_i) \\in {0,1}\\) where 0 denotes present, 1 denotes absent.\n\nSimilarity function\n\nDot Product: \\(\\sum x_i y_i\\)\nThis indicates the number of distinct query words matched in d\n\nProblems\n\ndisregard the word counts\ndisregard the importance of words\n\n\n\n\nVSM with TF-IDF weighting\nThis combines TF and IDF to address two problems of the simplest VSM.\n\nTerm Frequency Vector\nThis incorporates the word counts into weight.\n\n\\(x_i\\) = count of word \\(W_i\\) in query \\(c(W_i,q)\\)\n\\(y_i\\) = count of word \\(W_i\\) in doc \\(c(W_i,d)\\)\n\n\n\nInverse Document Frequency\nThis incorporates the importance of words into weight.\n\n\\(x_i = c(W_i,q)\\)\n\\(y_i = c(W_i, d) * IDF(W_i)\\) where \\(IDF(W) = log[\\frac{M+1}{df(w)}]\\), M = number of docs, \\(df(w)\\) = number of docs containing W. The larger \\(df(w)\\), the smaller IDF.\n\nHowever, term frequency has too much influence on weight.\n\n\n\nSub-linear TF Transformation\nThis aims to cap the influence of TF on weight as follows.\n\\[\ny_i = \\frac{(k+1)c(w,d)}{c(w,d)+k} < k + 1 (k >= 0)\n\\]\nTherefore,\n\\[\nf(q,d) = \\sum x_i y_i = \\sum c(w,q) \\frac{(k+1)c(w,d)}{c(w,d)+k} log{\\frac{M+1}{df(w)}}\n\\]\n\n\nDocument Length Normalization\nPenalize a long doc as follows.\n\\[\nnormalizer = 1 -b + b \\frac{|d|}{avdl}\n\\]\nwhere \\(avdl\\) = average doc length and \\(b \\in [0,1]\\).\nHere are two functions with document length normalization.\n\nPivoted Length Normalization VSM\n\\[\nf(q,d) = \\sum c(w,q) \\frac{ln[1 + ln[1 + c(w,d)]]}{1 - b + b \\frac{|d|}{avdl}} log \\frac{M+1}{df(w)}\n\\]\n\n\nBM25/Okapi\n\\[\nf(q,d) = \\sum c(w,q) \\frac{(k+1)c(w,d)}{c(w,d) + k(1-b+b\\frac{|d|}{avdl})} log \\frac{M+1}{df(w)}\n\\]\nwhere \\(b \\in [0,1]\\) and \\(k \\in [0, + \\infty)\\).\n\\(\\frac{(k+1)x}{x+k}\\) is the basic sub-linear transformation for \\(x\\) with the upper limit \\(k+1\\). \\(\\frac{(k+1)x}{norm*(x+k)}\\) changes the upper limit with respect to \\(norm\\). On the other hand, \\(\\frac{(k+1)x}{x+norm*k}\\) does not change the upper limit of \\(k+1\\).\nYou can play with this. https://www.desmos.com/calculator"
  },
  {
    "objectID": "text_retrieval.html#system-implementation",
    "href": "text_retrieval.html#system-implementation",
    "title": "Text Retrieval",
    "section": "System Implementation",
    "text": "System Implementation\n\nThe flow of system process\n\nTokenization\n\nnormalize lexical units\nstemming: mapping all inflectional forms to the same root form\n\nIndexing\n\nConvert documents to data structure\nInverted index is most popular.\n\nIt prepares the following two data structures.\n\nDictionary (lexicon): modest size in memory\n\nTerm\n# docs containing the term\ntotal freq\n\nPostings: huge size on disk\n\nTerm id\nDoc id\nTerm freq\nterm position in doc\n\n\nConstruct inverted index by sort-based methods.\n\nParse docs & Count: (termID, docID, freq) sorted by docID\nLocal sort: (termID, docID, freq) sorted by termID among each document\nMerge Sort: (termID, docID, freq) sorted by termID and docID among all documents\nOutput invert index file.\n\nCompress inverted index, exploiting Zipf’s Law\n\nTF compression\n\nassign fewer bits for frequent terms\n\nDocID compression\n\nd-gap: store difference like d1, d2-d1, d3-d2,…\n\nThis is feasible due to sequential access of documents\n\n\nInteger compression methods\n\nBinary: equal-length coding\nUnary: x>=1 is coded as x-1 one bits followed by 0\n\n3 = 110\n5 = 11110\n\n\\(\\gamma\\)-code: x = unary code for \\(1+ k\\) followed by uniform code for \\(x-2^k\\)in \\(k\\) bits where \\(k = \\lfloor log_2 x \\rfloor\\).\n\n3 = 101 where \\(k = 1\\)\n5 = 11001 where \\(k=2\\)\n\n\\(\\delta\\)-code: x = \\(\\gamma\\)-code code for \\(1+ k\\) followed by uniform code for \\(x-2^k\\)in \\(k\\) bits where \\(k = \\lfloor log_2 x \\rfloor\\).\n\n3 = 1001 where \\(k_\\delta = 1, k_\\gamma = 1\\)\n5 = 10101 where \\(k_\\delta = 2, k_\\gamma = 1\\)\n\n\n\nScoring\n\n\\(f(q,d) = f_a(h(g(t_1,d,q),…,g(t_k,d,q)),f_d(d),f_q(q))\\)\n\\(f_a(), f_d(d),f_q(q)\\) are final score adjustments.\n\\(f_d(d),f_q(q)\\) are pre-computed.\n\n\n\n\n\n\nZipf’s Law\nA few words occur very frequently while most occur rarely.\nWord rank by frequency * frequency \\(\\sim\\) constant\n\\[\nr(w)^\\alpha \\times F(w) = C\n\\]\nwhere \\(\\alpha \\sim 1, C \\sim 0.1\\)."
  },
  {
    "objectID": "text_retrieval.html#evaluation",
    "href": "text_retrieval.html#evaluation",
    "title": "Text Retrieval",
    "section": "Evaluation",
    "text": "Evaluation\n\nWhat to measure\n\nEffectiveness/Accuracy\nEfficiency\nUsability\n\n\n\nThe Cranfield Evaluation Methodology\n\nBuild reusable test collections & define measures\n\nA sample collection of documents\nA sample set of queries/topics\nRelevance judgments by human assessors\n\nThese samples are reusable for different systems.\n\nCompare the human judgement with the system output.\n\n\n\n\nQuantify the evaluation\n\nPrecision = True Positive / (True Positive +False Positive)\nRecall= True Positive / (True Positive + False Negative)\nF-Measure: combine precision and recall\n\n\\(F_\\beta = \\frac{(\\beta^2 + 1)PR}{\\beta^2 P+R}\\)\n\\(F_1 = \\frac{2PR}{P+R}\\)\nRecall is considered \\(\\beta\\) times as inmportant as precision. I.e., the higher \\(\\beta\\), the more recall affects \\(F_\\beta\\).\n\n\\(F_0 = P\\)\n\\(F_\\infty = R\\)\n\n\nPrecision-Recall (PR) curve\n\nAverage Precision\n\n\\(AP = \\int_{0}^{1} p(r)dr\\)\nAP is the area under the precision-recall curve.\nThe average precision at every cutoff where a new relevant document is retrieved, divided by the total number of relevant documents.\n\nMean Average Precisions (MAP)\n\nEvaluate the system based on \\(n\\) query results.\nMAP: arithmetic mean of average precision. \\(\\frac{1}{n} \\sum_{i=1}^{n} AP_i\\)\ngMAP: geometric mean of average precision. \\((\\Pi_{i=1}^{n} AP_i)^{\\frac{1}{n}}\\)\ngMAP is more sensitive to low values and a good indicator of how the system performs in the presence of “hard” queries.\n\nMean Reciprocal Rank for the case with only one relevant document\n\nAverage Precision = \\(\\frac{1}{r}\\) where r is the rank position of the single relevant doc\n\nNormalized Discounted Cumulative Gain (nDCG)\n\nApplicable to multi-level judgements\n\ne.g., 1=non-relevant, 2=marginally relevant, 3=very relevant\n\n\\(DCG@10 = \\sum_{i=1}^{n} (\\frac{G_i}{\\log i})\\)\n\nYou can compare DCG of different systems as long as they evaluate the same query results.\n\n\\(IdealDCG@10 = \\max(\\sum_{i=1}^{n} (\\frac{G_i}{\\log i}))\\)\n\\(nDCG@10 = \\frac{DCG@10}{IdealDCG@10}\\)\n\nWe can compare nDCG among different topic and queries, since nDCG ranges from 0 to 1.\n\n\n\nStatistical Significance Tests\n\nSign Test\nWilcoxon\n\nPooling: no need for judging all the documents\n\nChoose a diverse set of ranking methods\nObtain top-K documents from each method\nCombine all the top-K documents to form a pool\nHuman assessors judge the documents in the pool.\nCompare each method’s score"
  },
  {
    "objectID": "text_retrieval.html#probabilistic-model",
    "href": "text_retrieval.html#probabilistic-model",
    "title": "Text Retrieval",
    "section": "Probabilistic Model",
    "text": "Probabilistic Model\n\\[\nf(d,q) = p(R=1|d,q)\n\\]\n\nClassic probabilistic model: BM25\nLanguage model: Query Likelihood. \\(p(R=1|d,q) \\sim p(q|d,R=1)\\).\nDivergence-from-randomness model: PL2\n\n\nUnigram LM: The Simplest Language Model\n\nGenerate text by generating each word independently.\n\n\\(p(w_1 w_2 … w_n) = p(w_1)p(w_2)…p(w_n)\\)\n\n\\(p(w|\\theta)=p(w|d) = \\frac{c(w,d)}{|d|}\\), where \\(\\theta\\) denotes a language model.\n\n\n\nAssociation Analysis\n\nTopic LM: \\(p(w|topic)\\)\nBackground LM: \\(p(w|B)\\) for words such as the, a, is, we\nNormalized Topic LM: \\(\\frac{p(w|topic)}{p(w|d)}\\)\n\nThis reduces the probability of common words and emphasizes the topic words.\n\n\n\n\nThe simplest ranking criterion\n\\(f(q,d) = \\log p(q|d) = \\sum \\log p(w_i|d) = \\sum c(w,q) \\log p(w|d)\\) where \\(p(q|d) = p(w_1|d) \\times … \\times p(w_n|d)\\)\n\n\nThe smoothed ranking criterion\nTo address \\(p(w|d) = 0\\), we should smooth the estimate.\n\\(p(w|d) = \\begin{cases} p_{seen}(w|d)\\\\ \\alpha_d p(w|C) \\end{cases}\\) to smooth LM, where \\(p_{seen}\\) is discounted ML estimate and \\(C\\) denotes a collection of language model.\nThen,\n\\[\n\\log p(q|d) = \\sum c(w,q) \\log p_{seen}(w|d) + \\sum_{c(w,d)=0} c(w,q) \\log (\\alpha_d p(w|C)) \\\\\n= \\sum_{c(w,d)>0} c(w,q) \\log \\frac{p_{seen}(w|d)}{\\alpha_d p(w|C)} + |q| \\log \\alpha_d + \\sum_{w \\in V} c(w,q) \\log p(w|C) \\\\\n= \\sum \\log \\frac{p_{seen}(w_i|d)}{\\alpha_d p(w_i|C)} + |q| \\log \\alpha_d + \\sum \\log p(w_i|C)\n\\]\nThis implies TF (\\(p_{seen}(w_i|d)\\)) -IDF (\\(\\frac{1}{p(w_i|C)}\\)) weighting + Document length normalization(\\(|q| \\log \\alpha_d\\)).\nAs for doc length normalization, we should set smaller \\(\\alpha_d\\) long documents because we need less smoothing. In this sense, this term penalizes the long documents.\nWe can ignore the last term for ranking since it is the same for all \\(p(q|d)\\).\n\n\nHow to determine \\(p_{seen}(w_i|d)\\) and \\(\\alpha_d\\)\nHere are two popular methods.\n\nLinear Interpolation (Jelinek-Mercer) Smoothing\n\\[\np(w|d) = (1-\\lambda) \\frac{c(w,d)}{|d|} + \\lambda p(w|C) \\\\\n\\alpha_d = \\lambda\n\\]\nwhere \\(\\lambda \\in [0,1]\\).\nTherefore , the ranking function is:\n\\[\nf_{JM}(q,d) = \\sum c(w,q) \\log \\frac{p_{seen}(w|d)}{\\alpha_d p(w|C)} + |q| \\log \\alpha_d \\\\\n= \\sum c(w,q) \\log[1 + \\frac{1-\\lambda}{\\lambda} \\frac{c(w,d)}{|d|p(w|C)}]\n\\]\nbecause we can ignore \\(n \\log \\lambda\\), which is independent from document.\nSo JM smoothing does not incorporate document length normalization.\n\n\nDirichlet Prior (Bayesian) Smoothing\n\\[\np(w|d) = \\frac{|d| \\frac{c(w,d)}{|d|} + \\mu p(w|C)}{|d| + \\mu}\\\\\n\\alpha_d = \\frac{\\mu}{|d| + \\mu}\n\\]\nwhere \\(\\mu \\in [0, +\\infty)\\).\nTherefore, the ranking function is:\n\\[\nf_{DIR}(q,d) = \\sum c(w,q) \\log [1 + \\frac{c(w,d)}{\\mu p(w|C)}] + |q| \\log \\frac{\\mu}{\\mu + |d|}\n\\]"
  },
  {
    "objectID": "text_retrieval.html#feedback",
    "href": "text_retrieval.html#feedback",
    "title": "Text Retrieval",
    "section": "Feedback",
    "text": "Feedback\nHere are two ways to obtain feedback.\n\nRelevance Feedback: users make explicit relevance judgments.\nPseudo/Blind/Automatic Feedback: Top-K results are assumed to be relevant. Least reliable.\nImplicit Feedback: User-clicked docs are assumed to be relevant\n\nWe can update ranking functions in the following two ways.\n\nAdding new weighted terms\nAdjusting weights of old terms\n\n\nRocchio Feedback for VSM\n\\[\nq_{new} = \\alpha q_{old} + \\frac{\\beta}{|D|} \\sum_{d_j=rel} d_j - \\frac{\\gamma}{|D|} \\sum_{d_j=non-rel} d_j\n\\]\n\n\nKullback-Leibler (KL) Divergence Retrieval Model for LM\nKL-divergence (cross entropy):\n\\[\nf(q,d) = \\sum p(w|\\hat{\\theta}_Q) \\log \\frac{p_{seen}(w|d)}{\\alpha_d p(w|C)} + |q| \\log \\alpha_d\n\\]\nKL-divergence retrival model is a generalization of query likelihood.\nQuery Likelihood:\n\\[\nf(q,d) = \\sum c(w,q) \\log \\frac{p_{seen}(w|d)}{\\alpha_d p(w|C)} + |q| \\log \\alpha_d\n\\]\n\\[\np(w|\\hat{\\theta}_Q) = \\frac{c(w,Q)}{|Q|}\n\\]\nDivergence denotes the one between query model \\(\\hat{\\theta}_Q\\) and document model \\(p_{seen}(w|d)\\).\nThen, give feedback documents \\(F={d_1,…,d_n}\\),\n\\[\n\\log p(F|\\theta) = \\sum_{i} \\sum_{w} c(w, d_i) \\log [(1-\\lambda)p(w|\\theta) + \\lambda p(w|C)]\n\\]\n\\[\n\\theta_F = \\arg \\max_{\\theta} \\log p(F|\\theta)\n\\]\nwhere \\(\\lambda\\) is the noise in feedback documents.\nFinally, we can update \\(\\theta_Q\\) as follows.\n\\[\n\\theta_Q' = (1-\\alpha)\\theta_Q + \\alpha \\theta_F\n\\]"
  },
  {
    "objectID": "text_retrieval.html#web-search",
    "href": "text_retrieval.html#web-search",
    "title": "Text Retrieval",
    "section": "Web Search",
    "text": "Web Search\n\nChallenges\n\nScalability: Parallel indexing & searching\n\nMapReduce: a framework for parallel computation. E.g., Hadoop\n\n\n\n\n\n\nNote we call reduce for each distinct key.\n\nGoogle File System (GFS): distributed file system\n\n\nGFS master\n\nreceive messages from client and instruct chunk server\nFixed chunk size: 64MB\n\nGFS chunk server\n\nsend data to client\n\nGFS client\n\n\nLow quality information and spams: Spam detection & Robust ranking\nDynamics of the Web\n\n\n\nMajor Crawling Strategies\n\nBreadth-First\nParallel crawling\nFocused crawling\n\ntarget at a subset of pages\ntypically given a query\n\nIncremental crawling\n\nCrawl more often for pages frequently updated and accessed.\n\n\n\n\nRanking\nWe can exploit\n\nclickthroughs for massive implicit feedback\nlinks\n\n\n\nExploit links\n\nAnchor text\nIt summarizes the linked page.\n\n\nPageRank\nIt is basically citation counting.\nRandom surfing enables to jump randomly:\n\nWith probability \\(\\alpha\\), randomly jumping to another page.\nWith probability \\(1-\\alpha\\), randomly picking a link to follow.\n\nThe PageRank score \\(p(d_j)\\) is defined as follows.\n\\[\np(d_j) = \\sum [\\frac{1}{N} \\alpha + (1-\\alpha) M_{ij}]p(d_i)\n\\]\nwhere \\(M_{ij}\\) is the transition matrix between each document, which is very sparse and efficient for calculation. We iterate this calculation until it converges.\nWe do not need to normalize \\(p\\) since normalization does not affect ranking.\n\n\nHITS (Hypertext-Induced Topic Search)\nA: Adjacency matrix with adjacent pages 1.\n\\(h(d_i)\\): Hub scores\n\\(a(d_i)\\): Authority scores\nThen iterate the following calculations until converge.\n\\[\nh(d_i) = \\sum_{d_j \\in OUT(d_i)} a(d_j)\n\\]\n\\[\na(d_i) = \\sum_{d_j \\in IN(d_i)} h(d_j)\n\\]\nHere, we should normalize \\(h\\) and \\(a\\) to make them equally influence.\n\n\n\nLearning to rank\n\\[\nP(R=1|Q,D) = s(X_1(Q,D),...,X_n(Q,D), \\lambda)\n\\]\nwhere\\(X_i\\) is a feature such as BM25 and PageRank, and \\(\\lambda\\) is a set of parameters.\n\nRegression-Based Approach\n\\[\n\\log \\frac{P(R=1|Q,D)}{1 - P(R=1|Q,D)} = \\beta_0 + \\sum \\beta_i X_i\n\\]\nOf course, we can attempt to directly optimize a retrieval measure such as MAP and nDCG, but this is more difficult as an optimization problem while there are many solutions proposed.\n\n\n\nThe Data-User-Service (DUS) Triangle\n\nData\n\nWeb pages\nNews articles\n\nUsers\n\nLawyers\nScientists\n\nService\n\nSearch\nBrowsing\nMining\nTask support"
  },
  {
    "objectID": "text_retrieval.html#recommendation",
    "href": "text_retrieval.html#recommendation",
    "title": "Text Retrieval",
    "section": "Recommendation",
    "text": "Recommendation\nThis is push mode.\n\nItem similarity: content-based filtering\n\nRecommend X which is similar to items U likes.\n\nUser similarity: collaborative filtering\n\nRecommend X which similar users like.\n\n\n\nContent-based filtering\n\nScore each doc based on each docs’ features and user interest profile (e.g., a query, ratings of items).\nDeliver only docs over the threshold to users.\nUser gives feedback on the linear utility of the delivered docs (e.g., clickthroughs).\n\nthe linear utility such as (3 * #good (clicked) - 2 * #bad (not clicked)).\n\nUpdate the vector to score docs and threshold to maximize the linear utility.\n\n\nBeta-Gamma Threshold Learning\nThe threshold learning has a problem in that we cannot obtain feedback under the threshold.\nTo address this issue, we set the threshold \\(\\theta\\) heuristically as\n\\[\n\\theta = \\alpha * \\theta_{zero} + (1-\\alpha)*\\theta_{optimal}\n\\]\nwhere \\(\\alpha = \\beta + (1-\\beta)e^{-N*\\gamma}\\), N= the number of training examples, \\(\\theta_{zero}\\) is threshold where utility equals 0.\nSo \\(\\alpha\\) gets smaller as we train with more examples, i.e., less exploration.\n\n\n\nCollaborative filtering\nThis makes filtering decisions for an individual user based on those of other users.\n\nGiven a user u, find similar users {u1,…,um}.\n\nUser similarity can be calculated from their similarity in preferences of items.\n\nPredict u’s preference based on the preferences of {u1,…,um}.\n\nSo this requires large number of user preferences at the beginning, otherwise we encounter a “cold start” problem.\nGiven these values,\n\n\\(X_{ij}\\): rating of item \\(o_j\\) by user \\(u_i\\)\n\\(n_i\\): average rating of all items by user \\(u_i\\)\n\\(V_{ij}\\): normalized ratings \\(X_{ij} - n_i\\)\n\\(w(a,i)\\): the similarity between user \\(u_a\\) and \\(u_i\\)\n\nwe can predict the rating of item \\(o_j\\) by user \\(u_a\\)\n\\[\n\\hat{v}_{aj} = \\frac{\\sum w(a,i) v_{ij}}{\\sum w(a,i)}\n\\]\n\\[\n\\hat{x}_{aj} = \\hat{v}_{aj} + n_a\n\\]\nThere are several ways to calculate user similarity.\nPearson correlation coefficient:\n\nCosine measure:"
  }
]