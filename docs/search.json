[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Introduction to Text Retrieval and Mining",
    "section": "",
    "text": "This is a note for the lectures CS 410 Text Information Systems."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "text_retrieval.html#text-access",
    "href": "text_retrieval.html#text-access",
    "title": "Text Retrieval",
    "section": "Text Access",
    "text": "Text Access\n\nPull: search engines\n\nQuerying\nBrowsing\n\nPush: recommender systems"
  },
  {
    "objectID": "text_retrieval.html#text-retrieval-problem",
    "href": "text_retrieval.html#text-retrieval-problem",
    "title": "Text Retrieval",
    "section": "Text Retrieval Problem",
    "text": "Text Retrieval Problem\nSearch engine system returns relevant documents to users from collection of text documents.\nOften called “information retrieval (IR),” but IR is much broader than text retrieval.\nTR is an empirically defined problem and cannot mathematically prove one method to be the best.\nWe have two methods, but generally prefer ranking.\n\nDocument selection\n\nReturn the docs which system decides as relevant (absolute relevance)\nCons\n\nThe classifier can be inaccurate\n\n“Over-constrained” query gets no relevant docs.\n“Under-constrained” query gets too many docs.\n\nCannot see the degree of relevance or cannot prioritize docs\n\n\nDocument ranking\n\nReturn the docs with a relevance measure exceeding a cutoff (relative relevance)"
  },
  {
    "objectID": "text_retrieval.html#text-retrieval-methods",
    "href": "text_retrieval.html#text-retrieval-methods",
    "title": "Text Retrieval",
    "section": "Text Retrieval Methods",
    "text": "Text Retrieval Methods\nHere are four funcitons to calculate relevance \\(f(q,d)\\), all of which tend to result in similar ranking functions.\n\nSimilarity-based models: \\(f(q,d) = similarity(q,d)\\)\n\nVector space model\n\nProbabilistic model: \\(f(d,q) = p(R=1|d,q)\\)\nProbabilistic inference model: \\(f(q,d) = p(d \\rightarrow q)\\)\nAxiomatic model: f(q,d) must satisfy a set of constraints\n\nThree variables:\n\nTerm Frequency (TF): \\(c(term, d)\\)\nDocument Length: \\(|d|\\)\nDocument Frequency (df): \\(P(term | entire \\space collection)\\).\n\nThe Best models:\n\nPivoted length normalization\nBM25: most popular\nQuery likelihood\nPL2"
  },
  {
    "objectID": "text_retrieval.html#vector-space-model-vsm",
    "href": "text_retrieval.html#vector-space-model-vsm",
    "title": "Text Retrieval",
    "section": "Vector Space Model (VSM)",
    "text": "Vector Space Model (VSM)\nThis model adopts similarity-based model to calculate relevance.\n\nTerm\n\nEach term defines one dimension\nN terms define an N-dimensional space\n\nQuery: \\(q = (x_1, …, x_N)\\) is query term weight.\nDoc: \\(d=(y_1,…,y_N)\\) is doc term weight.\n\n\nSimplest VSM\n\nDimension\n\nBag of Words: \\(V=(w_1,…,w_n)\\)\nA text (such as a sentence or a document) is represented as the bag of its words.\nIt focuses on individual words.\nIt discards\n\nphrases by multiple words\nword ordering\n\n\nWeight\n\nBit Vector: \\((x_i, y_i) \\in {0,1}\\) where 0 denotes present, 1 denotes absent.\n\nSimilarity function\n\nDot Product: \\(\\sum x_i y_i\\)\nThis indicates the number of distinct query words matched in d\n\nProblems\n\ndisregard the word counts\ndisregard the importance of words\n\n\n\n\nVSM with TF-IDF weighting\nThis combines TF and IDF to address two problems of the simplest VSM.\n\nTerm Frequency Vector\nThis incorporates the word counts into weight.\n\n\\(x_i\\) = count of word \\(W_i\\) in query \\(c(W_i,q)\\)\n\\(y_i\\) = count of word \\(W_i\\) in doc \\(c(W_i,d)\\)\n\n\n\nInverse Document Frequency\nThis incorporates the importance of words into weight.\n\n\\(x_i = c(W_i,q)\\)\n\\(y_i = c(W_i, d) * IDF(W_i)\\) where \\(IDF(W) = log[\\frac{M+1}{df(w)}]\\), M = number of docs, \\(df(w)\\) = number of docs containing W. The larger \\(df(w)\\), the smaller IDF.\n\nHowever, term frequency has too much influence on weight.\n\n\n\nSub-linear TF Transformation\nThis aims to cap the influence of TF on weight as follows.\n\\[\ny_i = \\frac{(k+1)c(w,d)}{c(w,d)+k} < k + 1 (k >= 0)\n\\]\nTherefore,\n\\[\nf(q,d) = \\sum x_i y_i = \\sum c(w,q) \\frac{(k+1)c(w,d)}{c(w,d)+k} log{\\frac{M+1}{df(w)}}\n\\]\n\n\nDocument Length Normalization\nPenalize a long doc as follows.\n\\[\nnormalizer = 1 -b + b \\frac{|d|}{avdl}\n\\]\nwhere \\(avdl\\) = average doc length and \\(b \\in [0,1]\\).\nHere are two functions with document length normalization.\n\nPivoted Length Normalization VSM\n\\[\nf(q,d) = \\sum c(w,q) \\frac{ln[1 + ln[1 + c(w,d)]]}{1 - b + b \\frac{|d|}{avdl}} log \\frac{M+1}{df(w)}\n\\]\n\n\nBM25/Okapi\n\\[\nf(q,d) = \\sum c(w,q) \\frac{(k+1)c(w,d)}{c(w,d) + k(1-b+b\\frac{|d|}{avdl})} log \\frac{M+1}{df(w)}\n\\]\nwhere \\(b \\in [0,1]\\) and \\(k \\in [0, + \\infty)\\)."
  },
  {
    "objectID": "text_retrieval.html#system-implementation",
    "href": "text_retrieval.html#system-implementation",
    "title": "Text Retrieval",
    "section": "System Implementation",
    "text": "System Implementation\n\nThe flow of system process\n\nTokenization\n\nnormalize lexical units\nstemming: mapping all inflectional forms to the same root form\n\nIndexing\n\nConvert documents to data structure\nInverted index is most popular.\n\nIt prepares the following two data structures.\n\nDictionary (lexicon): modest size in memory\n\nTerm\n# docs containing the term\ntotal freq\n\nPostings: huge size on disk\n\nTerm id\nDoc id\nTerm freq\nterm position in doc\n\n\nConstruct inverted index by sort-based methods.\n\nParse docs & Count: (termID, docID, freq) sorted by docID\nLocal sort: (termID, docID, freq) sorted by termID among each document\nMerge Sort: (termID, docID, freq) sorted by termID and docID among all documents\nOutput invert index file.\n\nCompress inverted index, exploiting Zipf’s Law\n\nTF compression\n\nassign fewer bits for frequent terms\n\nDocID compression\n\nd-gap: store difference like d1, d2-d1, d3-d2,…\n\nThis is feasible due to sequential access of documents\n\n\nInteger compression methods\n\nBinary: equal-length coding\nUnary: x>=1 is coded as x-1 one bits followed by 0\n\n3 = 110\n5 = 11110\n\n\\(\\gamma\\)-code: x = unary code for \\(1+ k\\) followed by uniform code for \\(x-2^k\\)in \\(k\\) bits where \\(k = \\lfloor log_2 x \\rfloor\\).\n\n3 = 101 where \\(k = 1\\)\n5 = 11001 where \\(k=2\\)\n\n\\(\\delta\\)-code: x = \\(\\gamma\\)-code code for \\(1+ k\\) followed by uniform code for \\(x-2^k\\)in \\(k\\) bits where \\(k = \\lfloor log_2 x \\rfloor\\).\n\n3 = 1001 where \\(k_\\delta = 1, k_\\gamma = 1\\)\n5 = 10101 where \\(k_\\delta = 2, k_\\gamma = 1\\)\n\n\n\nScoring\n\n\\(f(q,d) = f_a(h(g(t_1,d,q),…,g(t_k,d,q)),f_d(d),f_q(q))\\)\n\\(f_a(), f_d(d),f_q(q)\\) are final score adjustments.\n\\(f_d(d),f_q(q)\\) are pre-computed.\n\n\n\n\n\n\nZipf’s Law\nA few words occur very frequently while most occur rarely.\nWord rank by frequency * frequency \\(\\sim\\) constant\n\\[\nr(w)^\\alpha \\times F(w) = C\n\\]\nwhere \\(\\alpha \\sim 1, C \\sim 0.1\\)."
  },
  {
    "objectID": "text_retrieval.html#evaluation",
    "href": "text_retrieval.html#evaluation",
    "title": "Text Retrieval",
    "section": "Evaluation",
    "text": "Evaluation\n\nWhat to measure\n\nEffectiveness/Accuracy\nEfficiency\nUsability\n\n\n\nThe Cranfield Evaluation Methodology\n\nBuild reusable test collections & define measures\n\nA sample collection of documents\nA sample set of queries/topics\nRelevance judgments by human assessors\n\nThese samples are reusable for different systems.\n\nCompare the human judgement with the system output.\n\n\n\n\nQuantify the evaluation\n\nPrecision = True Positive / (True Positive +False Positive)\nRecall= True Positive / (True Positive + False Negative)\nF-Measure: combine precision and recall\n\n\\(F_\\beta = \\frac{(\\beta^2 + 1)PR}{\\beta^2 P+R}\\)\n\\(F_1 = \\frac{2PR}{P+R}\\)\nThe higher \\(\\beta\\), the more recall affects \\(F_\\beta\\).\n\n\\(F_0 = P\\)\n\\(F_\\infty = R\\)\n\n\nPrecision-Recall (PR) curve\n\nAverage Precision\n\n\\(AP = \\int_{0}^{1} p(r)dr\\)\nAP is the area under the precision-recall curve.\nThe average precision at every cutoff where a new relevant document is retrieved, divided by the total number of relevant documents.\n\nMean Average Precisions (MAP)\n\nEvaluate the system based on \\(n\\) query results.\nMAP: arithmetic mean of average precision. \\(\\frac{1}{n} \\sum_{i=1}^{n} AP_i\\)\ngMAP: geometric mean of average precision. \\((\\Pi_{i=1}^{n} AP_i)^{\\frac{1}{n}}\\)\ngMAP is more sensitive to low values and a good indicator of how the system performs in the presence of “hard” queries.\n\nMean Reciprocal Rank for the case with only one relevant document\n\nAverage Precision = \\(\\frac{1}{r}\\) where r is the rank position of the single relevant doc\n\nNormalized Discounted Cumulative Gain (nDCG)\n\nApplicable to multi-level judgements\n\ne.g., 1=non-relevant, 2=marginally relevant, 3=very relevant\n\n\\(DCG@10 = \\sum_{i=1}^{n} (\\frac{G_i}{\\log i})\\)\n\nYou can compare DCG of different systems as long as they evaluate the same query results.\n\n\\(IdealDCG@10 = \\max(\\sum_{i=1}^{n} (\\frac{G_i}{\\log i}))\\)\n\\(nDCG@10 = \\frac{DCG@10}{IdealDCG@10}\\)\n\nWe can compare nDCG among different topic and queries, since nDCG ranges from 0 to 1.\n\n\n\nStatistical Significance Tests\n\nSign Test\nWilcoxon\n\nPooling: no need for judging all the documents\n\nChoose a diverse set of ranking methods\nObtain top-K documents from each method\nCombine all the top-K documents to form a pool\nHuman assessors judge the documents in the pool.\nCompare each method’s score"
  },
  {
    "objectID": "text_retrieval.html#probabilistic-model",
    "href": "text_retrieval.html#probabilistic-model",
    "title": "Text Retrieval",
    "section": "Probabilistic Model",
    "text": "Probabilistic Model\n\\[\nf(d,q) = p(R=1|d,q)\n\\]\n\nClassic probabilistic model: BM25\nLanguage model: Query Likelihood. \\(p(R=1|d,q) \\sim p(q|d,R=1)\\).\nDivergence-from-randomness model: PL2\n\n\nUnigram LM: The Simplest Language Model\n\nGenerate text by generating each word independently.\n\n\\(p(w_1 w_2 … w_n) = p(w_1)p(w_2)…p(w_n)\\)\n\n\\(p(w|\\theta)=p(w|d) = \\frac{c(w,d)}{|d|}\\), where \\(\\theta\\) denotes a language model.\n\n\n\nAssociation Analysis\n\nTopic LM: \\(p(w|topic)\\)\nBackground LM: \\(p(w|B)\\) for words such as the, a, is, we\nNormalized Topic LM: \\(\\frac{p(w|topic)}{p(w|d)}\\)\n\nThis reduces the probability of common words and emphasizes the topic words.\n\n\n\n\nThe simplest ranking criterion\n\\(f(q,d) = \\log p(q|d) = \\sum \\log p(w_i|d) = \\sum c(w,q) \\log p(w|d)\\) where \\(p(q|d) = p(w_1|d) \\times … \\times p(w_n|d)\\)\n\n\nThe smoothed ranking criterion\nTo address \\(p(w|d) = 0\\), we should smooth the estimate.\n\\(p(w|d) = \\begin{cases} p_{seen}(w|d)\\\\ \\alpha_d p(w|C) \\end{cases}\\) to smooth LM, where \\(p_{seen}\\) is discounted ML estimate and \\(C\\) denotes a collection of language model.\nThen,\n\\[\n\\log p(q|d) = \\sum c(w,q) \\log p_{seen}(w|d) + \\sum_{c(w,d)=0} c(w,q) \\log (\\alpha_d p(w|C)) \\\\\n= \\sum_{c(w,d)>0} c(w,q) \\log \\frac{p_{seen}(w|d)}{\\alpha_d p(w|C)} + |q| \\log \\alpha_d + \\sum_{w \\in V} c(w,q) \\log p(w|C) \\\\\n= \\sum \\log \\frac{p_{seen}(w_i|d)}{\\alpha_d p(w_i|C)} + |q| \\log \\alpha_d + \\sum \\log p(w_i|C)\n\\]\nThis implies TF (\\(p_{seen}(w_i|d)\\)) -IDF (\\(\\frac{1}{p(w_i|C)}\\)) weighting + Document length normalization(\\(|q| \\log \\alpha_d\\)).\nAs for doc length normalization, we should set smaller \\(\\alpha_d\\) long documents because we need less smoothing. In this sense, this term penalizes the long documents.\nWe can ignore the last term for ranking since it is the same for all \\(p(q|d)\\).\n\n\nHow to determine \\(p_{seen}(w_i|d)\\) and \\(\\alpha_d\\)\nHere are two popular methods.\n\nLinear Interpolation (Jelinek-Mercer) Smoothing\n\\[\np(w|d) = (1-\\lambda) \\frac{c(w,d)}{|d|} + \\lambda p(w|C) \\\\\n\\alpha_d = \\lambda\n\\]\nwhere \\(\\lambda \\in [0,1]\\).\nTherefore , the ranking function is:\n\\[\nf_{JM}(q,d) = \\sum c(w,q) \\log \\frac{p_{seen}(w|d)}{\\alpha_d p(w|C)} + |q| \\log \\alpha_d \\\\\n= \\sum c(w,q) \\log[1 + \\frac{1-\\lambda}{\\lambda} \\frac{c(w,d)}{|d|p(w|C)}]\n\\]\nbecause we can ignore \\(n \\log \\lambda\\), which is independent from document.\nSo JM smoothing does not incorporate document length normalization.\n\n\nDirichlet Prior (Bayesian) Smoothing\n\\[\np(w|d) = \\frac{|d| \\frac{c(w,d)}{|d|} + \\mu p(w|C)}{|d| + \\mu}\\\\\n\\alpha_d = \\frac{\\mu}{|d| + \\mu}\n\\]\nwhere \\(\\mu \\in [0, +\\infty)\\).\nTherefore, the ranking function is:\n\\[\nf_{DIR}(q,d) = \\sum c(w,q) \\log [1 + \\frac{c(w,d)}{\\mu p(w|C)}] + |q| \\log \\frac{\\mu}{\\mu + |d|}\n\\]"
  },
  {
    "objectID": "text_retrieval.html#feedback",
    "href": "text_retrieval.html#feedback",
    "title": "Text Retrieval",
    "section": "Feedback",
    "text": "Feedback\nHere are two ways to obtain feedback.\n\nRelevance Feedback: users make explicit relevance judgments.\nPseudo/Blind/Automatic Feedback: Top-K results are assumed to be relevant. Least reliable.\nImplicit Feedback: User-clicked docs are assumed to be relevant\n\nWe can update ranking functions in the following two ways.\n\nAdding new weighted terms\nAdjusting weights of old terms\n\n\nRocchio Feedback for VSM\n\\[\nq_{new} = \\alpha q_{old} + \\frac{\\beta}{|D|} \\sum_{d_j=rel} d_j - \\frac{\\gamma}{|D|} \\sum_{d_j=non-rel} d_j\n\\]\n\n\nKullback-Leibler (KL) Divergence Retrieval Model for LM\nKL-divergence (cross entropy):\n\\[\nf(q,d) = \\sum p(w|\\hat{\\theta}_Q) \\log \\frac{p_{seen}(w|d)}{\\alpha_d p(w|C)} + |q| \\log \\alpha_d\n\\]\nKL-divergence retrival model is a generalization of query likelihood.\nQuery Likelihood:\n\\[\nf(q,d) = \\sum c(w,q) \\log \\frac{p_{seen}(w|d)}{\\alpha_d p(w|C)} + |q| \\log \\alpha_d\n\\]\n\\[\np(w|\\hat{\\theta}_Q) = \\frac{c(w,Q)}{|Q|}\n\\]\nDivergence denotes the one between query model \\(\\hat{\\theta}_Q\\) and document model \\(p_{seen}(w|d)\\).\nThen, give feedback documents \\(F={d_1,…,d_n}\\),\n\\[\n\\log p(F|\\theta) = \\sum_{i} \\sum_{w} c(w, d_i) \\log [(1-\\lambda)p(w|\\theta) + \\lambda p(w|C)]\n\\]\n\\[\n\\theta_F = \\arg \\max_{\\theta} \\log p(F|\\theta)\n\\]\nwhere \\(\\lambda\\) is the noise in feedback documents.\nFinally, we can update \\(\\theta_Q\\) as follows.\n\\[\n\\theta_Q' = (1-\\alpha)\\theta_Q + \\alpha \\theta_F\n\\]"
  },
  {
    "objectID": "text_retrieval.html#web-search",
    "href": "text_retrieval.html#web-search",
    "title": "Text Retrieval",
    "section": "Web Search",
    "text": "Web Search\n\nChallenges\n\nScalability: Parallel indexing & searching\n\nMapReduce: a framework for parallel computation. E.g., Hadoop\n\n\n\n\n\n\nNote we call reduce for each distinct key.\n\nGoogle File System (GFS): distributed file system\n\n\nGFS master\n\nreceive messages from client and instruct chunk server\nFixed chunk size: 64MB\n\nGFS chunk server\n\nsend data to client\n\nGFS client\n\n\nLow quality information and spams: Spam detection & Robust ranking\nDynamics of the Web\n\n\n\nMajor Crawling Strategies\n\nBreadth-First\nParallel crawling\nFocused crawling\n\ntarget at a subset of pages\ntypically given a query\n\nIncremental crawling\n\nCrawl more often for pages frequently updated and accessed.\n\n\n\n\nRanking\nWe can exploit\n\nclickthroughs for massive implicit feedback\nlinks\n\n\n\nExploit links\n\nAnchor text\nIt summarizes the linked page.\n\n\nPageRank\nIt is basically citation counting.\nRandom surfing enables to jump randomly:\n\nWith probability \\(\\alpha\\), randomly jumping to another page.\nWith probability \\(1-\\alpha\\), randomly picking a link to follow.\n\nThe PageRank score \\(p(d_j)\\) is defined as follows.\n\\[\np(d_j) = \\sum [\\frac{1}{N} \\alpha + (1-\\alpha) M_{ij}]p(d_i)\n\\]\nwhere \\(M_{ij}\\) is the transition matrix between each document, which is very sparse and efficient for calculation. We iterate this calculation until it converges.\nWe do not need to normalize \\(p\\) since normalization does not affect ranking.\n\n\nHITS (Hypertext-Induced Topic Search)\nA: Adjacency matrix with adjacent pages 1.\n\\(h(d_i)\\): Hub scores\n\\(a(d_i)\\): Authority scores\nThen iterate the following calculations until converge.\n\\[\nh(d_i) = \\sum_{d_j \\in OUT(d_i)} a(d_j)\n\\]\n\\[\na(d_i) = \\sum_{d_j \\in IN(d_i)} h(d_j)\n\\]\nHere, we should normalize \\(h\\) and \\(a\\) to make them equally influence.\n\n\n\nLearning to rank\n\\[\nP(R=1|Q,D) = s(X_1(Q,D),...,X_n(Q,D), \\lambda)\n\\]\nwhere\\(X_i\\) is a feature such as BM25 and PageRank, and \\(\\lambda\\) is a set of parameters.\n\nRegression-Based Approach\n\\[\n\\log \\frac{P(R=1|Q,D)}{1 - P(R=1|Q,D)} = \\beta_0 + \\sum \\beta_i X_i\n\\]\nOf course, we can attempt to directly optimize a retrieval measure such as MAP and nDCG, but this is more difficult as an optimization problem while there are many solutions proposed.\n\n\n\nThe Data-User-Service (DUS) Triangle\n\nData\n\nWeb pages\nNews articles\n\nUsers\n\nLawyers\nScientists\n\nService\n\nSearch\nBrowsing\nMining\nTask support"
  },
  {
    "objectID": "text_retrieval.html#recommendation",
    "href": "text_retrieval.html#recommendation",
    "title": "Text Retrieval",
    "section": "Recommendation",
    "text": "Recommendation\nThis is push mode.\n\nItem similarity: content-based filtering\n\nRecommend X which is similar to what items U likes.\n\nUser similarity: collaborative filtering\n\nRecommend X which similar users like.\n\n\n\nContent-based filtering\n\nScore each doc based on each docs’ features and user interest profile (e.g., a query, ratings of items).\nDeliver only docs over the threshold to users.\nUser gives feedback on the linear utility of the delivered docs (e.g., clickthroughs).\n\nthe linear utility such as (3 * #good (clicked) - 2 * #bad (not clicked)).\n\nUpdate the vector to score docs and threshold to maximize the linear utility.\n\n\nBeta-Gamma Threshold Learning\nThe threshold learning has a problem in that we cannot obtain feedback under the threshold.\nTo address this issue, we set the threshold \\(\\theta\\) heuristically as\n\\[\n\\theta = \\alpha * \\theta_{zero} + (1-\\alpha)*\\theta_{optimal}\n\\]\nwhere \\(\\alpha = \\beta + (1-\\beta)e^{-N*\\gamma}\\), N= the number of training examples, \\(\\theta_{zero}\\) is threshold 0.\nSo \\(\\alpha\\) gets smaller as we train with more examples, i.e., less exploration.\n\n\n\nCollaborative filtering\nThis makes filtering decisions for an individual user based on those of other users.\n\nGiven a user u, find similar users {u1,…,um}.\n\nUser similarity can be calculated from their similarity in preferences of items.\n\nPredict u’s preference based on the preferences of {u1,…,um}.\n\nSo this requires large number of user preferences at the beginning, otherwise we encounter a “cold start” problem.\nGiven these values,\n\n\\(X_{ij}\\): rating of item \\(o_j\\) by user \\(u_i\\)\n\\(n_i\\): average rating of all items by user \\(u_i\\)\n\\(V_{ij}\\): normalized ratings \\(X_{ij} - n_i\\)\n\\(w(a,i)\\): the similarity between user \\(u_a\\) and \\(u_i\\)\n\nwe can predict the rating of item \\(o_j\\) by user \\(u_a\\)\n\\[\n\\hat{v}_{aj} = \\frac{\\sum w(a,i) v_{ij}}{\\sum w(a,i)}\n\\]\n\\[\n\\hat{x}_{aj} = \\hat{v}_{aj} + n_a\n\\]\nThere are several ways to calculate user similarity.\nPearson correlation coefficient:\n\nCosine measure:"
  }
]